---
layout: post
title: Finding Fake News with TensorFlow
---

## Loading and Preparing Data

This blog post will focus on creating a machine learning model with TensorFlow that can predict whether an article is "fake news" based on its contents. The data we'll be using has already been split into the train and test sets, so for now, we'll just load in the training data.

```python
import pandas as pd
import tensorflow as tf
import numpy as np

train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
fake_news = pd.read_csv(train_url)
```

Before any models can be made, there are a couple things we have to do to prepare the data.

Our models will be looking for connections between words to figure out if an article is fake. Stopwords, like a, the, or, etc. might throw off the model, so we'll remove them from both article titles and text. The `nltk` library has the list of stopwords in English that we'll be needing.


```python
#gets list of stop words in English
from nltk.corpus import stopwords
stop_words = stopwords.words("english")

def remove_stop_words(s):
    """
    Removes stop words from given string
    parameter s: the string with stop words to be removed
    returns a string with common stop words taken out
    """
    s2 = ""
    for word in s.split():
        if word not in stop_words:
            s2 += word + " "
    return s2
```
Next we have to convert our `pandas` DataFrame into a TensorFlow dataset. This dataset will have two inputs (the predictor variables) and one output (the target variable). After the dataset is created, the rows will be batched together in groups of 100 to make model training faster later on.

```python
def make_dataset(df):
    """
    Removes stop words from title and text columns, then constructs a 
    TensorFlow dataset with (title, text) as inputs and fake as output
    parameter df: dataframe to be turned into dataset
    returns a tf.data.Dataset as above
    """
    #applies the remove_stop_words function to each line in the title/text columns
    df["title"] = df["title"].apply(remove_stop_words)
    df["text"] = df["text"].apply(remove_stop_words)
    #creates the dataset with the right inputs and output
    data = tf.data.Dataset.from_tensor_slices((
        {"title": df[["title"]],  "text": df["text"]}, 
        {"fake" : df[["fake"]]}
    ))
    #batch in rows of 100 to make training easier later on
    data = data.batch(100)
    return data
```
From this training data, we're going to split off 20% of the rows into a validation set. The validation set basically exists to get an early idea of how well the model is doing as it's training, since the test data shouldn't be touched until the very end.

```python
data = make_dataset(fake_news)

#shuffles the data to randomly assign articles to train or validation sets
data = data.shuffle(buffer_size = len(data))
#split in a ratio of 80 train to 20 validation
train_size = int(0.8*len(data))
val_size = int(0.2*len(data))
train = data.take(train_size)
val = data.skip(train_size).take(val_size)
```

## Creating Models

It's time to start creating the models! We'll be making 3 in total. The first and second will have identical layers, but model 1 only takes in the article title, while model 2 only takes in the article text. The final model will use both title and text to predict whether the article is fake. Each model will take in data, pass the data through several layers, then output its prediction.

Our models will be using the Keras `Functional` API, and we first need to specify the two inputs we'll be using. 

```python
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import losses

title_in = keras.Input(
    shape = (1,), #one title per article
    name = "title",
    dtype = "string"
)
text_in = keras.Input(
    shape = (1,), #once text body per article
    name = "text",
    dtype = "string"
)
```
Now we'll write 2 functions to process the contents of the article into quantitative data that the models can process. The first function, `standardization` will make all words lowercase and remove any punctuation. 

The `text_vectorizer` function will create a TextVectorization layer, then adapt it to the contents of either the title or text. After adapting, `vectorize_layer` will have a vector indicating the frequency rank of each word. Words that appear often will have higher ranks than those that only appear a few times.

```python
import re
import string
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
size_vocabulary = 2000

#taken from Week 7 Monday
def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

def text_vectorizer(name): #either "title" or "text"
    vectorize_layer = TextVectorization(
        standardize=standardization, #standardize text
        max_tokens=2000, 
        output_mode='int',
        #sets the max length of the vector
        #any words past the 500th will be cut off and unused
        output_sequence_length=500) 
    vectorize_layer.adapt(train.map(lambda x, y: x[name]))
    return vectorize_layer
```

### Models 1 and 2: Article Title/Text

The function below will make the first two models we'll be training. Data first gets passed through the text vectorizer, then it reaches an embedding layer that will be important later on. It passes through a few more layers before reaching the output layer. This output layer has the same name ("fake") as the output in the Dataset created in `make_dataset`, which lets the model know what it should compare to when checking accuracy.

Once the layer pipeline is completed, the model is created with the specified input and output. Then it gets compiled, where we pick the optimization algorithm ("adam") and loss function our model should use. Since we're doing classification, we'll be using categorial cross-entropy. We also specify that we want our model to show the accuracy of each step as its training.

```python
def make_model(column, input):
    """
    Write layers to process data, then create a model and compile it. 
    This model only takes in one input, either the article title or text
    parameter column: name of the column used in model ("title" or "text")
    parameter input: either title_in or text_in
    return a compiled model ready for training
    """
    #layer sequence
    features = text_vectorizer(column)(input)
    features = layers.Embedding(size_vocabulary, 3, name = "embedding")(features)
    features = layers.Dense(8, activation = 'relu')(features)
    features = layers.GlobalAveragePooling1D()(features)  
    output = layers.Dense(2, name = "fake")(features)
 
    #create and compile model
    model = keras.Model(inputs = [input], outputs = output)
    model.compile(optimizer = "adam",
                loss = losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])
    return model
```
Our first model will be making predictions just based off of the article title. All models in this post will be training for 15 epochs. Increasing the number of epochs can sometimes increase accuracy, but our models are pretty simple and fully train relatively quickly, so there's no need to make it any longer.

```python
model_title = make_model("title", title_in)
history_title = model_title.fit(train, validation_data=val, epochs = 15, verbose = False)
```
Let's go ahead and plot how the accuracy is changing at each epoch as our model trains. Since we'll be doing this multiple times, we might as well write a quick function for it.

```python
from matplotlib import pyplot as plt

def plot_accuracy(history, plot_title):
    """
    Plots accuracy of model across epochs
    parameter history: history returned from training model
    parameter plot_title: the title of the plot produced
    """
    plt.plot(history.history["accuracy"], label = "train")
    plt.plot(history.history["val_accuracy"], label = "val")
    plt.gca().set(title = plot_title, xlabel = "Epoch", ylabel = "Training Accuracy")
    plt.legend()

plot_accuracy(history_title, "Article Title")
```
![]({{christinegu27.github.io}}/images/output_20_0.png)
    
From the graph, we can see that the model reaches an accuracy of more than 95% using just the article title. We'll do the same steps as before with our second model, which takes on the article text.

```python
model_text= make_model("text", text_in)
history_text = model_text.fit(train, validation_data=val, epochs = 15, verbose = False)
plot_accuracy(history_text, "Article Text")
```

![]({{christinegu27.github.io}}/images/output_22_1.png)
    
Like the first model, our second model also does a really good job of predicting whether an article is fake or not based on its contents.

```python
print("Validation accuracy of title column:", history_title.history["val_accuracy"][-1])
print("Validation accuracy of text column:", history_text.history["val_accuracy"][-1])
```

    Validation accuracy of title column: 0.9746666550636292
    Validation accuracy of text column: 0.9710047245025635
    
Our models can accurately predict whether an article is fake about 97 to 98% of the time. Since the 2 models have almost identical accuracy, for now, we'll stick with the second model as our "winner" so far. (It's very possible that rerunning the code could lead to model 2 having better accuracy than model 1 since they're so close, which was the case when originally writing this post.)

### Model 3: Article Title and Text

The final model will be taking in both the article title and text. Unlike the other two which were basically a straight line from start to finish, this model is going to have two inputs that merge together in the model. The title and text will both be vectorized, then passed through a shared embedder before being merged.

```python
#name embedding layer so it can be used for both inputs
shared_embed = layers.Embedding(size_vocabulary, 3, name = "embedding")

title_features = text_vectorizer("title")(title_in)
title_features = shared_embed(title_features)

text_features = text_vectorizer("text")(text_in)
text_features = shared_embed(text_features)
```
Once the two inputs are concatenated together, we'll pass it through a few more layers before reaching the output layer to increase accuracy of the model. 

```python
main = layers.concatenate([title_features, text_features], axis = 1)
main = layers.Dense(16, activation = 'relu')(main)
main = layers.GlobalAveragePooling1D()(main)
output = layers.Dense(2, name = "fake")(main)

model = keras.Model(inputs = [title_in, text_in], outputs = output)
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
```
After compiling the model, we can actually plot it to get a visualization of the different layers and how they connect to each other.

```python
keras.utils.plot_model(model)
```
  
![]({{christinegu27.github.io}}/images/output_32_0.png)
    
And now we'll train the model, then plot its accuracy over the epochs just like the other two models.

```python
history_both = model.fit(train, validation_data=val, epochs = 15, verbose = False)
plot_accuracy(history_both, "Both Article Title and Text")
```
  
![]({{christinegu27.github.io}}/images/output_34_0.png)
    
```python
print("Validation accuracy of both columns:", history_both.history["val_accuracy"][-1])
```

    Validation accuracy of both columns: 0.9657777547836304
    

## Model Evaluation

All 3 of the models are able to predict if an article is fake above 95% of the time, which is great! Our best model will be the second one, which makes predictions based only on the article text. Let's now test it on the test data, which we first have to load in, then convert into a Dataset using `make_dataset` from earlier.

```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test = pd.read_csv(test_url)
#creates a tf dataset from the test data
test_data = make_dataset(test)
```
```python
#makes our text_only model
model_test = make_model("text", text_in)
#trains model on the test_data
history_test = model_test.fit(test_data, epochs = 15)
```

    Epoch 1/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.6876 - accuracy: 0.5507
    Epoch 2/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.5928 - accuracy: 0.8012
    Epoch 3/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.4026 - accuracy: 0.9172
    Epoch 4/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.2907 - accuracy: 0.9395
    Epoch 5/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.2334 - accuracy: 0.9502
    Epoch 6/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.1982 - accuracy: 0.9553
    Epoch 7/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.1739 - accuracy: 0.9587
    Epoch 8/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.1559 - accuracy: 0.9629
    Epoch 9/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.1419 - accuracy: 0.9660
    Epoch 10/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.1305 - accuracy: 0.9687
    Epoch 11/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.1211 - accuracy: 0.9711
    Epoch 12/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.1132 - accuracy: 0.9730
    Epoch 13/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.1064 - accuracy: 0.9750
    Epoch 14/15
    225/225 [==============================] - 4s 17ms/step - loss: 0.1005 - accuracy: 0.9765
    Epoch 15/15
    225/225 [==============================] - 4s 16ms/step - loss: 0.0953 - accuracy: 0.9773
    
After training for 15 epochs, our model is able to get an accuracy of 97.7%, which again, is very good. Since the test accuracy matches up with the accuracy we got on the training data, we can be happy knowing there also wasn't any overfitting.

## Embedding Visualization

Now that we have succesfully created a model with accuracy, it may be interesting to see what kind of words it associates with each other, and also the "fakeness" of the article. We can do this with the embedding layer that was included in the model earlier. In this embedding layer, words get weights that we can plot to get a visualization of the words in the article.

```python
#taken from Week 6  Friday 

#gets weights of words used in the model
weights = model_test.get_layer('embedding').get_weights()[0] 
#gets the list of words used 
vocab = text_vectorizer("text").get_vocabulary() 
weights
```

    array([[ 0.02872383,  0.0058405 ,  0.01013284],
           [ 0.32788196,  0.31211668,  0.46277618],
           [-2.2376661 ,  2.2642746 , -2.137367  ],
           ...,
           [ 0.58889663,  0.4812144 ,  0.5155641 ],
           [-0.8934293 ,  0.7368872 , -0.8469697 ],
           [ 0.39673856,  0.42501384,  0.44070855]], dtype=float32)

The collection of weights is currently 3-dimensional because one of the parameters to the embedding layer was `3` back when we were creating the layer sequence. Since we're plotting in 2-D, we'll have to convert the weights. We'll be using principle component analysis (PCA) from `sklearn` to do this. After that's done, we'll turn the word and its weight into a dataframe

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)

#transforms weights 2-D from 3-D
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
embedding_df.head()
```
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>-0.238257</td>
      <td>-0.469737</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>0.335345</td>
      <td>-0.255021</td>
    </tr>
    <tr>
      <th>2</th>
      <td>said</td>
      <td>-2.944111</td>
      <td>2.273521</td>
    </tr>
    <tr>
      <th>3</th>
      <td>trump</td>
      <td>1.442927</td>
      <td>0.322795</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the</td>
      <td>-1.539942</td>
      <td>0.885191</td>
    </tr>
  </tbody>
</table>
</div>
Now we can create our interactive word embedding by plotting columns from the dataframe with `plotly`. Words that are located close to each other means the model found connections between them.

```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```
{% include embedding.html %}

(note to people reading the draft: This doesn't look right so let me know if I did anything wrong thanks)
