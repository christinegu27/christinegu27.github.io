---
layout: post
title: Spectral Clustering
---

## Introduction

This blog post will focus on creating a function that will perform **spectral clustering** on a set of data. As usual, let's import some of the required packages first. `numpy` and `pyplot` should be self explanatory by now, but the `datasets` package from `sklearn` is what we'll use to create our data that takes on certain shapes when graphed.
```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```
So what is clustering exactly? *Clustering* is basically a method to partition up a set of data into different groups, or clusters. K-means clustering was introduced back in PIC 16A, and an example can be seen below. 

```python
from sklearn.cluster import KMeans
n = 200
np.random.seed(1111)

#X holds the Euclidean coordinates of each point
#y holds each point's cluster label 
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![K-means example on "circular" data]({{christinegu27.github.io}}/images/kmeans_ex.png)
    
We can see that the data is clustered around two sets, so k-means had no trouble picking out which group each data point belongs in. However, it doesn't do so well when the data is shaped more strangely -- i.e. not in circular blobs.

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
 
![K-means example on "crescent" data]({{christinegu27.github.io}}/images/kmeans_moons.png)
    
While we can see that the data follows two different clusters, the k-means algorithm is clearly having some trouble. By design, it's looking for circular clusters but in this case, the shape of each cluster is a crescent. In the following parts, we will work towards a function that will be able to assign cluster lables to data points, even if they take some more interesting shapes.

## Part A: The Similarity Matrix

First, we will create the similarity matrix, $$\mathbf{A}$$. $$\mathbf{A}$$ will be a matrix of n rows and n columns, where n is the number of data points we have. Entries of $$\mathbf{A}$$ will be determined based on these rules:

* `A[i,i]`, so entries along the diagonal, is equal to 0
* `A[i,j]` is equal to **1** if the distance between `X[i]` (the coordinates of data point `i`) and `X[j]` (the coordinates of data point `j`) is **less than** `epsilon`, or **0** otherwise

`epsilon` is a rough threshold for how far apart we think points in the same cluster are. For this blog post, we will mainly stick with `epsilon = 0.4`. To get the distance between points, we'll use the `pairwise_distances` function from `sklearn.metrics`, which finds the distance between each possible pair of points in a dataset (`X`) and returns them in a very convenient matrix of size (nxn). We will then use numpy array slicing to set appropriate values in our similarity matrix `A`.

```python
from sklearn import metrics
#gets a matrix with euclidean distance between all points in X
distance = metrics.pairwise_distances(X)

#sets the i,jth entry of A to be 1 if the distance is less than epsilon
epsilon = 0.4
A = (distance <= epsilon).astype(int)

#sets diagonal entries to 0
np.fill_diagonal(A,0)
A
```

    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 1., 0.],
           ...,
           [0., 0., 0., ..., 0., 1., 1.],
           [0., 0., 1., ..., 1., 0., 1.],
           [0., 0., 0., ..., 1., 1., 0.]])
{::options parse_block_html="true" /}
<div class="got-help">
Defined matrix <b>A</b> (and later <b>D</b>) directly instead of creating an empty matrix and filling values after.
</div>
{::options parse_block_html="false" /}

To understand this matrix, let's look at the last row of this matrix. The very last entry, `A[199,199]`, represents the distance between the 200th point and itself, which is obviously equal to 0. The distance between the 199th point in X and the 200th point must be less than `epsilon`, since `A[199,198]` is equal to 1. The distance between the first point and the 200th point must be more than `epsilon`, since `A[199,0]` is off the diagonal and equal to 0.

## Part B: The Norm Cut Objective

For this assignment, each data point either belongs to cluster $$C_0$$ or $$C_1$$. If `y[i] == 0`, then data point `i` belongs to $$C_0$$. 

The *binary norm cut objective* (shortened to normcut from here out) will tell us how well our two clusters $$C_0$$ and $$C_1$$ do as partitions of the dataset. The smaller the normcut for our clusters, the better job they do partitioning the data. The formula is as follows:

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

Ok that formula is a lot to take in, so let's try and break it down a little.

#### The Cut Term

For a pair of points `X[i]` and `X[j]`, $$\mathbf{cut}(C_0, C_1)$$ is the total number of pairs in `X` where `X[i]` and `X[j]` belong to different clusters, but the distance between them is less than `epsilon`. If the cut is small, then we can say that in general, points in $$C_0$$ are pretty far from points in $$C_1$$.

Remember that we can think of the row `i` of `A` as the "current" point, and the entries in the columns `j` of `A` are the "distances" from our current point `i` to the other 199 points in `X`. To actually find the cut term, we have to sum up all the entries in `A` where the cluster of point `i` doesn't match the `j`-th point. We'll write a function called `cut` that takes in a similarity matrix `A` and an array `y` holding the cluster labels to do this.

```python
def cut(A,y):
    cut_sum = 0
    #goes through all entries in A
    for i in range(len(y)):
        for j in range(len(y)):
            #checks if entry is linking between points in different clusters
            if (y[i] != y[j]):
                cut_sum += A[i,j]
    return cut_sum
```

Let's find the cut of our clusters as defined in `y`.

```python
print("Cut of true clusters:", cut(A,y))
```

    Cut of true clusters: 26.0
    
Not bad! This means there are relatively few points in `X` that are close to one another but belong to different clusters. And for comparison, let's create an array of randomized labels. This is basically randomly assigning labels to our points in `X` without any regard to the point's coordinates (so ignoring which crescent it is a part of).

```python
random_clusters = np.random.randint(2, size = n)
print("Cut of randomized clusters:",cut(A, random_clusters))
```

    Cut of randomized clusters: 2290.0

As expected, these random clusters have a much higher cut than the actual clusters. This means there are lots of points that are close to one another, but belong to different clusters.

#### The Volume Term 

Volume is literally how big each cluster is. The actual definition looks complicated but just think that if a point is in $$C_0$$, then it is adding to the volume, or size, of $$C_0$$. Here's the mathematical definition:
* Let $$d_i = \sum_{j = 1}^n a_{ij}$$. Then $$\mathbf{vol}(C_0)$$ is defined as $$\sum_{i \in C_0}d_i$$.

There's a lot of symbols here, but the actual function to find the volume of $$C_0$$ and $$C_1$$ is pretty straightforward.

```python
def vols(A,y):
    #gets rows where the first element is in cluster 0
    c_0 = A[y==0,:]
    #gets rows where the first element is in cluster 1
    c_1 = A[y==1,:]
    #returns a tuple with the sum of # of elements in each cluster
    return c_0.sum(), c_1.sum()
```
#### The Norm Cut

Now that we have the cut and the volume of each cluster, it's time to substitute these values back into the formula for the norm cut. 

```python
def normcut(A,y):
    vol_0, vol_1 = vols(A,y)
    cut_term = cut(A,y)
    #uses the formula to compute the binary norm cut
    return cut_term*(1/vol_0 + 1/vol_1)
```
```python
print("True clusters:", normcut(A,y))
print("Randomized clusters:", normcut(A, random_clusters))
```

    True clusters: 0.02303682466323045
    Randomized clusters: 2.0283758914171135
    
The normcut for our true clusters is much, much lower than the normcut for the randomized clusters. We can safely conclude that the true cluster labels contained in `y` are a better partition of the data than the randomized labels we created. 

## Part C: Matrix Multiplication

With our new normcut defined, we could theoretically find some cluster vector `y` that minimizes the value returned by `normcut(A,y)`. Unfortunately, this is extremely inefficient so we're going to need a better approach.

Here's something we'll try instead. Let $$\mathbf{z}$$ be a vector (numpy array) defined as

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$

Also let $$\mathbf{D}$$ be a diagonal matrix where the nonzero entries along the diagonal are equal to $$d_i = \sum_{j = 1}^n a_{ij}$$ (the same $$d_i$$ defined earlier). From the way we constructed $$\mathbf{z}$$ and $$\mathbf{D}$$, it turns out that there is another way to define the normcut, 

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;.$$

To verify this, we'll first write a function that creates $$\mathbf{z}$$ from `A` and `y` using the formula above.

```python
def transform(A,y):
    vol_0, vol_1 = vols(A,y)
    #sets all elements in z to 1/vol_0
    z = np.full(n, 1/vol_0)
    #sets ith element in z to -1/vol_1 if element is in cluster 1
    z[y == 1] = -1/vol_1
    return z
```

The next lines of code create $$\mathbf{D}$$, then carry out the matrix multiplication. Note that $$\mathbf{z}$$ is automatically transposed during the multiplication, so there's no need to put `z.T`.
```python
z = transform(A,y)
#an array holding the row_sums
d_i = A.sum(axis = 1)

#creating diagonal matrix defined above
D = np.diag(d_i)

normcut_1 = normcut(A,y) #normcut from the function
normcut_2 = 2 * (z@(D-A)@z)/(z@D@z) #from formula above

normcut_1, normcut_2
```

    (0.02303682466323045, 0.023036824663230177)

We can see that these two values are basically the same. The small discrepancy between the two is because of the limitations of computer arithmetic, where numbers get rounded or cut off after a certain number of decimal points.

The way we defined $$\mathbf{z}$$ effectively encodes all the info that was stored in `y` earlier. A point `X[i]` is in cluster $$C_0$$ if `y[i]==0`, which is now equivalent to "`X[i]` is in $$C_0$$ if `z[i] > 0`". 

One identity we can verify is $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 
```python
# uses 'isclose' instead of '==' due to computing limitations
np.isclose(z@D@(np.ones(n)),0)
```

    True

This tells us that $$\mathbf{z}$$ has an equal number of positive and negative elements, so the data in `X` is split evenly between clusters $$C_0$$ and $$C_1$$.

## Part D: Minimizing the Normcut

We want to minimize $$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}$$ subject to the constraint that $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. The `orth_obj` function represents this minimization problem.

```python
def orth(u, v):
    return (u @ v) / (v @ v)*v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```
To then minimize `orth_obj` with respect to `z`, we will use the `minimize` function from `scipy.optimize`. The "Nelder-Mead" algorithm is used for solving since `minimize` won't work without it.
{::options parse_block_html="true" /}

<div class="gave-help">
Suggested that they change the solving method from the default to properly split clusters based on sign
</div>
{::options parse_block_html="false" /}

```python
from scipy.optimize import minimize

z_min = minimize(orth_obj, z, method="Nelder-Mead")
```
Recall that the sign of an entry in z represents which cluster it belongs in, so we'll use that for color-coding.

```python
#Part E in the assignment
plt.scatter(X[:,0], X[:,1], c = z_min.x<0)
```
![Clustering by minimizing the normcut]({{christinegu27.github.io}}/images/optimized.png)

This looks very good! Each crescent represents a different cluster, and our graph correctly colors the points different colors based on which crescent it belongs to.
    
## Part E: Eigenvalues and Eigenvectors

Running the minimize function a few lines above actually took some time, so we still need to find an efficient way to find this $$\mathbf{z}$$. We'll actually perform spectral clustering by solving for the eigenvalues and eigenvectors of the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. This matrix is called the *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. Once we find them, the eigenvector corresponding with the second-smallest eigenvalue is the one holding our cluster labels. 

```python
#create the Laplacian matrix for A
L = (np.linalg.inv(D))@(D-A)

#get the eigenvalues and corresponding eigenvectors of L
eigval, eigvec = np.linalg.eig(L)

#sort the eigenvalues and corresponding eigenvectors from smallest to largest 
ix = eigval.argsort()
eigval, eigvec = eigval[ix], eigvec[:,ix]

#gets eigenvector corresponding with the second smallest eigenvalue
z_eig = eigvec[:,1]

plt.scatter(X[:,0], X[:,1], c = z_eig<0)
```
![Clustering with eigenvectors]({{christinegu27.github.io}}/images/moons_evec.png)

While the clustering isn't perfect, it's still pretty good and more importantly, way faster to compute than the vector given by minimization from earlier. 

Why do we use the second smallest eigenbalue? If we had used the eigenvector corresponding with the smallest eigenvalue, then it would just be the vector $$\mathbb{1}$$, or the vector of just 1s. As you can probably imagine, this vector won't be helpful at all for assigning cluster labels.

## Part F: The Final Function

It's finally time to put everything from the previous parts together into one `spectral_clustering` function. This functions will:
1. Create the similarity matrix for the data based on some `epsilon`
2. Create the Laplacian matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$ of $$\mathbf{A})$$
3. Solve for eigenvalues and eigenvectors of $$\mathbf{L}$$, then find the eigenvector corresponding to the second smallest eigenvalue.
4. Give a vector of labels based on the sign of elements in the eigenvector. 

```python
def spectral_clustering(X, epsilon):
    """
    Performs binary spectral clustering on a set of data
    parameter X: an array holding the points to be labeled
    parameter epsilon: the distance threshold between points in X
    returns an array indicating which cluster each point in X belongs
    in
    """
    #getting distances between all points in X
    distance = metrics.pairwise_distances(X)
    #constructing similarity matrix based in part A
    A = (distance <= epsilon).astype(int)
    np.fill_diagonal(A,0)
   
    #creating diagonalized matrix D with row sums of A in the diagonals
    D = np.diag(A.sum(axis = 1))
    #constructing the Laplcian matrix of A
    L = np.linalg.inv(D)@(D-A)
    
    #finds eigenvalues and eigenvectors of Laplacian matrix
    eigval, eigvec = np.linalg.eig(L)
    #sorts eigenvectors based on eigvalues sorted from small to large
    eigvec = eigvec[:, eigval.argsort()]
    #gets the eigenvector paired with 2nd smallest eigenvalue
    z_eig = eigvec[:,1]
    #assigns values of 0 or 1 based on sign of elements
    return (z_eig < 0).astype(int)
```
{::options parse_block_html="true" /}
<div class="got-help">
Added more comments explaining the code in the function
</div>
{::options parse_block_html="false" /}

```python
plt.scatter(X[:,0], X[:,1], c=spectral_clustering(X, epsilon = 0.4))
```
![Spectral clustering ]({{christinegu27.github.io}}/images/moons_evec.png)

## Part G: Some Examples

Let's see how our new spectral_clustering function does with different datasets. Since our function is so efficient, we can increase the number of points in `X` to 1000 for better visualization. 

If we increase the noise, the points will become more spread out instead of following the crescent shape as closely. Since the points in each cluster are not as close to one another, we can expect that the clustering won't be as accurate.

```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c=spectral_clustering(X, epsilon = 0.4))
```
![noise = 0.1]({{christinegu27.github.io}}/images/noise_0.1.png)

With the noise increasing from 0.05 to 0.1, the clustering has started to make more mistakes at the end of each crescent, but overall it's still correctly coloring each cluster.

```python
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.15, random_state=None)
plt.scatter(X[:,0], X[:,1], c=spectral_clustering(X, epsilon = 0.4))
```
![noise = 0.15]({{christinegu27.github.io}}/images/noise_0.15.png)

Now with noise = 0.15, the shape of each crescent is becoming more abstract as the points start to spread away from the pattern. Our function is having some trouble with points at the end of the crescent since these points are now getting mixed in with the middle points of the other crescent. The clusters are starting to look more circular, which is not what we want. To fix this, let's reduce the value of `epsilon`. By setting a smaller value, we're telling the code that we expect the points in each cluster to be even closer to one another.

```python
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.15, random_state=None)
plt.scatter(X[:,0], X[:,1], c=spectral_clustering(X, epsilon = 0.25))
```
![noise = 0.15 and epsilon = 0.25]({{christinegu27.github.io}}/images/noise_0.15_epsilon.png)

Even though the points haven't moved, we can see that by changing epsilon, the clusters now better follow the shape of each cluster.

##### Part I from the assignment

To go even further, let's test our spectral clustering on another data pattern.
```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)

plt.scatter(X[:,0], X[:,1])
```
![]({{christinegu27.github.io}}/images/bullseye_graph.png)

Now our data forms a bull's eye where each circle is a different cluster. Since the data isn't nice grouped together in different blobs apart from each other, k-means isn't going to a great job.
```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![K-means clustering]({{christinegu27.github.io}}/images/bullseye_kmeans.png)

However the `spectral_clustering` function we created should be able to tell the circles apart because there's a decent amount of white space between them.

```python
plt.scatter(X[:,0], X[:,1], c=spectral_clustering(X, epsilon = 0.4), cmap = "tab10")
```
!["Spectral clustering"]({{christinegu27.github.io}}/images/bullseye_spectral.png) 

As expected, each cluster has successfully been found by spectral clustering, and they each get their own pretty shade of blue. From further testing, `epsilon = 0.53` seems to be about the cutoff point, where any `epsilon` 0.53 doesn't cluster the data correctly.

```python
plt.scatter(X[:,0], X[:,1], c=spectral_clustering(X, epsilon = 0.54), cmap = "tab10")
```
!["Spectral clustering"]({{christinegu27.github.io}}/images/bullseye_spectral_2.png)

{::options parse_block_html="true" /}
<div class="gave-help">
Generally gave feedback for classmates to take out lines with the directions from the prompt since they looked out of place in a blog post.
</div>
{::options parse_block_html="false" /}